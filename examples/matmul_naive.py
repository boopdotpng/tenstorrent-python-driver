#!/usr/bin/env python3
"""Multi-core matrix multiplication: C[M,N] = A[M,K] @ B[K,N]

NAIVE IMPLEMENTATION - each core computes a subset of output tiles.
For each output tile, it reads all required A and B tiles along the K dimension.

This achieves ~3 TFLOPS on 2048x2048, which is DRAM bandwidth bound.
The fast path (~140+ TFLOPS) requires:
1. Multicast - one core reads, broadcasts to many (reduces redundant DRAM reads)
2. 2D tiling - reuse A rows across output columns, B cols across output rows
3. L1 sharding - keep working set in L1, not DRAM
4. Double buffering - overlap reads with compute

See tt-metal's matmul_multicore_reuse_mcast_2d_program_factory.cpp for the fast path.
"""
from __future__ import annotations
import sys; sys.path.insert(0, str(__import__('pathlib').Path(__file__).parent.parent))
import time, struct, random
from codegen import Compiler, DataFormat, CkernelConfig, MathFidelity
from device import Device, Program, TileGrid
from dram import tilize, untilize

# Matrix dimensions (must be multiples of 32 for tiles)
M, K, N = 2048, 2048, 2048
Mt, Kt, Nt = M // 32, K // 32, N // 32
NUM_OUTPUT_TILES = Mt * Nt

# === Dataflow Kernels ===
# Runtime args for reader: [a_addr, b_addr, start_tile, num_tiles]
# Runtime args for writer: [c_addr, num_tiles, start_tile]
# Runtime args for compute: [num_tiles]

# Hardcode dimensions directly in kernels (will be auto-generated by tinygrad)
K_READER = f"""
#include <cstdint>

void kernel_main() {{
  uint32_t a_addr = get_arg_val<uint32_t>(0);
  uint32_t b_addr = get_arg_val<uint32_t>(1);
  uint32_t start_tile = get_arg_val<uint32_t>(2);
  uint32_t num_tiles = get_arg_val<uint32_t>(3);

  constexpr uint32_t Kt = {Kt};
  constexpr uint32_t Nt = {Nt};

  constexpr uint32_t cb_a = tt::CBIndex::c_0;
  constexpr uint32_t cb_b = tt::CBIndex::c_1;
  const uint32_t tile_size_a = get_tile_size(cb_a);
  const uint32_t tile_size_b = get_tile_size(cb_b);
  const InterleavedAddrGenFast<true> a_gen = {{
    .bank_base_address = a_addr, .page_size = tile_size_a, .data_format = DataFormat::Float16_b,
  }};
  const InterleavedAddrGenFast<true> b_gen = {{
    .bank_base_address = b_addr, .page_size = tile_size_b, .data_format = DataFormat::Float16_b,
  }};

  for (uint32_t tile = 0; tile < num_tiles; ++tile) {{
    uint32_t out_tile = start_tile + tile;
    uint32_t out_row = out_tile / Nt;
    uint32_t out_col = out_tile % Nt;

    for (uint32_t kt = 0; kt < Kt; ++kt) {{
      // A tile at (out_row, kt)
      uint32_t a_tile = out_row * Kt + kt;
      cb_reserve_back(cb_a, 1);
      noc_async_read_tile(a_tile, a_gen, get_write_ptr(cb_a));
      noc_async_read_barrier();
      cb_push_back(cb_a, 1);

      // B tile at (kt, out_col)
      uint32_t b_tile = kt * Nt + out_col;
      cb_reserve_back(cb_b, 1);
      noc_async_read_tile(b_tile, b_gen, get_write_ptr(cb_b));
      noc_async_read_barrier();
      cb_push_back(cb_b, 1);
    }}
  }}
}}
"""

K_WRITER = """
#include <cstdint>

void kernel_main() {
  uint32_t c_addr = get_arg_val<uint32_t>(0);
  uint32_t num_tiles = get_arg_val<uint32_t>(1);
  uint32_t start_tile = get_arg_val<uint32_t>(2);

  constexpr uint32_t cb_out = tt::CBIndex::c_16;
  const uint32_t tile_size = get_tile_size(cb_out);
  const InterleavedAddrGenFast<true> c_gen = {
    .bank_base_address = c_addr, .page_size = tile_size, .data_format = DataFormat::Float16_b,
  };

  uint32_t end_tile = start_tile + num_tiles;
  for (uint32_t tile = start_tile; tile < end_tile; ++tile) {
    cb_wait_front(cb_out, 1);
    noc_async_write_tile(tile, c_gen, get_read_ptr(cb_out));
    noc_async_write_barrier();
    cb_pop_front(cb_out, 1);
  }
}
"""

K_COMPUTE = f"""
#include <cstdint>
#include "compute_kernel_api/matmul.h"
#ifdef TRISC_MATH
  #include "ckernel_ops.h"
  #include "cmath_common.h"
  #include "ckernel_template.h"
#endif

namespace NAMESPACE {{
void MAIN {{
  uint32_t num_tiles = get_arg_val<uint32_t>(0);
  constexpr uint32_t Kt = {Kt};

  constexpr tt::CBIndex cb_a = tt::CBIndex::c_0;
  constexpr tt::CBIndex cb_b = tt::CBIndex::c_1;
  constexpr tt::CBIndex cb_out = tt::CBIndex::c_16;

  // mm_init programs ADDR_MOD registers and MOP replay buffer with MVMUL sequences
  mm_init(cb_a, cb_b, cb_out);

  for (uint32_t i = 0; i < num_tiles; ++i) {{
    tile_regs_acquire();
    for (uint32_t kt = 0; kt < Kt; ++kt) {{
      cb_wait_front(cb_a, 1);
      cb_wait_front(cb_b, 1);

      // Unpack: load tiles from CBs into SrcA/SrcB (TRISC0 only)
      // Note: in0 (cb_a) -> SrcB, in1 (cb_b) -> SrcA (hardware swaps internally)
      UNPACK((llk_unpack_AB_matmul(cb_a, cb_b, 0, 0)));

#ifdef TRISC_MATH
      // Set DST write address for tile 0 (with double-buffer offset)
      ckernel::math::set_dst_write_addr<DstTileShape::Tile32x32, UnpackDestination::SrcRegs>(0);

      // Execute the MOP which replays the MVMUL instruction sequence:
      //   16x TTI_MVMUL across 4 faces: D[8,16] = B[8,16] * A[16,16]
      //   with ADDR_MOD auto-incrementing srca/srcb/dest pointers
      //   final MVMUL clears SrcA (CLR_A) since ct_dim >= rt_dim (reuse_a)
      ckernel_template::run();

      // Reset SrcB for next tile pair (end of reuse boundary)
      TTI_SETRWC(p_setrwc::CLR_B, 0, 0, 0, 0, p_setrwc::SET_ABD_F);
#endif

      cb_pop_front(cb_a, 1);
      cb_pop_front(cb_b, 1);
    }}
    tile_regs_commit();
    tile_regs_wait();
    cb_reserve_back(cb_out, 1);
    pack_tile(0, cb_out);
    cb_push_back(cb_out, 1);
    tile_regs_release();
  }}
}}
}}  // namespace NAMESPACE
"""

def bf16_from_f32(x: float) -> int:
  return struct.unpack("<I", struct.pack("<f", x))[0] >> 16

def f32_from_bf16(x: int) -> float:
  return struct.unpack("<f", struct.pack("<I", (x & 0xFFFF) << 16))[0]

def make_bf16_matrix(rows: int, cols: int, seed: int) -> tuple[bytes, list[float]]:
  """Returns (bf16_bytes, f32_values) for verification."""
  r = random.Random(seed)
  f32_vals = [r.uniform(-1.0, 1.0) for _ in range(rows * cols)]
  buf = bytearray(rows * cols * 2)
  for i, v in enumerate(f32_vals):
    buf[i*2:(i+1)*2] = bf16_from_f32(v).to_bytes(2, "little")
  return bytes(buf), f32_vals

def matmul_ref(a: list[float], b: list[float], m: int, k: int, n: int) -> list[float]:
  """Reference CPU matmul."""
  c = [0.0] * (m * n)
  for i in range(m):
    for j in range(n):
      acc = 0.0
      for kk in range(k):
        acc += a[i * k + kk] * b[kk * n + j]
      c[i * n + j] = acc
  return c

def main():
  print(f"Matmul: C[{M},{N}] = A[{M},{K}] @ B[{K},{N}]")
  print(f"Tiles: Mt={Mt}, Kt={Kt}, Nt={Nt}, total_output_tiles={NUM_OUTPUT_TILES}")

  # Compile - LoFi is 3.6x faster than HiFi4, HiFi2 is 2x faster
  cfg = CkernelConfig(
    input_format=DataFormat.Float16_b,
    output_format=DataFormat.Float16_b,
    math_fidelity=MathFidelity.LoFi,  # Max speed
  )
  kernels = Compiler(cfg).compile(K_READER, K_WRITER, K_COMPUTE)

  device = Device()
  num_cores = len(TileGrid.TENSIX)
  print(f"Using {num_cores} cores")

  try:
    tile_bytes = 32 * 32 * 2  # bf16

    # Create input matrices
    print("Creating input matrices...")
    a_rm, a_f32 = make_bf16_matrix(M, K, seed=42)
    b_rm, b_f32 = make_bf16_matrix(K, N, seed=123)

    # Tilize and upload
    print("Uploading to DRAM...")
    a_tiled = tilize(a_rm, 2, rows=M, cols=K)
    b_tiled = tilize(b_rm, 2, rows=K, cols=N)
    a_buf = device.dram.alloc_write(a_tiled, name="A", page_size=tile_bytes)
    b_buf = device.dram.alloc_write(b_tiled, name="B", page_size=tile_bytes)
    c_buf = device.dram.alloc(tile_bytes * NUM_OUTPUT_TILES, name="C", page_size=tile_bytes)

    # Work distribution: split output tiles across cores
    use_all_cores = True
    if use_all_cores:
      active_cores = min(num_cores, NUM_OUTPUT_TILES)
      tiles_per_core = (NUM_OUTPUT_TILES + active_cores - 1) // active_cores
      cores = TileGrid.TENSIX[:active_cores]
    else:
      # Single core does all tiles
      tiles_per_core = NUM_OUTPUT_TILES
      active_cores = 1
      cores = [TileGrid.TENSIX[0]]

    def reader_args(core_idx: int, core_xy: tuple[int,int], n_cores: int) -> list[int]:
      start = core_idx * tiles_per_core
      count = min(tiles_per_core, NUM_OUTPUT_TILES - start)
      if start >= NUM_OUTPUT_TILES: count = 0
      return [a_buf.addr, b_buf.addr, start, count]

    def writer_args(core_idx: int, core_xy: tuple[int,int], n_cores: int) -> list[int]:
      start = core_idx * tiles_per_core
      count = min(tiles_per_core, NUM_OUTPUT_TILES - start)
      if start >= NUM_OUTPUT_TILES: count = 0
      return [c_buf.addr, count, start]

    def compute_args(core_idx: int, core_xy: tuple[int,int], n_cores: int) -> list[int]:
      start = core_idx * tiles_per_core
      count = min(tiles_per_core, NUM_OUTPUT_TILES - start)
      if start >= NUM_OUTPUT_TILES: count = 0
      return [count]

    program = Program(
      reader=kernels.reader,
      writer=kernels.writer,
      compute=kernels.compute,
      reader_rt_args=reader_args,
      writer_rt_args=writer_args,
      compute_rt_args=compute_args,
      cbs=[0, 1, 16],
      tile_size=tile_bytes,
      num_pages=2,
      cores=cores,
    )

    print("Running matmul...")
    t = device.run(program)
    flops = 2 * M * N * K
    print(f"TFLOPS (compute only): {flops / t.compute / 1e12:.2f}, TFLOPS (total): {flops / t.total / 1e12:.2f}")

    # Read result
    print("Reading result...")
    c_tiled = device.dram.read(c_buf)
    c_rm = untilize(c_tiled, 2, rows=M, cols=N)

    # Reference computation (only for small matrices)
    if M * N <= 1024 * 1024:
      print("Computing reference...")
      c_ref = matmul_ref(a_f32, b_f32, M, K, N)

      c_got = []
      for i in range(0, len(c_rm), 2):
        c_got.append(f32_from_bf16(int.from_bytes(c_rm[i:i+2], "little")))

      # Compute PCC
      mean_ref = sum(c_ref) / len(c_ref)
      mean_got = sum(c_got) / len(c_got)
      num = sum((r - mean_ref) * (g - mean_got) for r, g in zip(c_ref, c_got))
      den_ref = sum((r - mean_ref) ** 2 for r in c_ref) ** 0.5
      den_got = sum((g - mean_got) ** 2 for g in c_got) ** 0.5
      pcc = num / (den_ref * den_got) if den_ref * den_got > 0 else 0.0

      print(f"PCC: {pcc:.6f}")
      if pcc < 0.97:
        raise SystemExit(f"PCC too low: {pcc:.6f} < 0.97")
    else:
      print("Skipping verification for large matrix")

    print("Test Passed")

  finally:
    device.close()

if __name__ == "__main__":
  main()
